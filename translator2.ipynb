{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Er-Ayush/PYTHON-MINI-PROJECTS/blob/main/translator2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNN7m7MpunIw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
        "from tensorflow.keras.layers import Dense, Layer\n",
        "from keras.backend import softmax\n",
        "\n",
        "from tensorflow import convert_to_tensor, string\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
        "from tensorflow.data import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
        "from tensorflow import math, cast, float32, linalg, ones, maximum, newaxis\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from numpy.random import shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow import convert_to_tensor, int64\n",
        "import pandas as pd\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow import data, train, math, reduce_sum, cast, equal, argmax, float32, GradientTape, TensorSpec, function, int64\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ye9y_SHu61e"
      },
      "outputs": [],
      "source": [
        "# Implementing the Scaled-Dot Product Attention\n",
        "class DotProductAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        " \n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        " \n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        " \n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = softmax(scores)\n",
        " \n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrpakNiXvEhU"
      },
      "outputs": [],
      "source": [
        "# Implementing the Multi-Head Attention\n",
        "class MultiHeadAttention(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
        "        self.heads = h  # Number of attention heads to use\n",
        "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
        "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
        "        self.d_model = d_model  # Dimensionality of the model\n",
        "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
        "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
        "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
        "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
        " \n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "        else:\n",
        "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
        "        return x\n",
        " \n",
        "    def call(self, queries, keys, values, mask=None):\n",
        "        # Rearrange the queries to be able to compute all heads in parallel\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange the keys to be able to compute all heads in parallel\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange the values to be able to compute all heads in parallel\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange back the output into concatenated form\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
        " \n",
        "        # Apply one final linear projection to the output to generate the multi-head attention\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "        return self.W_o(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAtsHdsEvTgv"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionEmbeddingFixedWeights(Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
        "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
        "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
        "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
        "        self.word_embedding_layer = Embedding(\n",
        "            input_dim=vocab_size, output_dim=output_dim,\n",
        "            weights=[word_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "        self.position_embedding_layer = Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim,\n",
        "            weights=[position_embedding_matrix],\n",
        "            trainable=False\n",
        "        )\n",
        "             \n",
        "    def get_position_encoding(self, seq_len, d, n=10000):\n",
        "        P = np.zeros((seq_len, d))\n",
        "        for k in range(seq_len):\n",
        "            for i in np.arange(int(d/2)):\n",
        "                denominator = np.power(n, 2*i/d)\n",
        "                P[k, 2*i] = np.sin(k/denominator)\n",
        "                P[k, 2*i+1] = np.cos(k/denominator)\n",
        "        return P\n",
        " \n",
        " \n",
        "    def call(self, inputs):        \n",
        "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
        "        embedded_words = self.word_embedding_layer(inputs)\n",
        "        embedded_indices = self.position_embedding_layer(position_indices)\n",
        "        return embedded_words + embedded_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH4WOwV3wYVY"
      },
      "outputs": [],
      "source": [
        "# Implementing the Add & Norm Layer\n",
        "class AddNormalization(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AddNormalization, self).__init__(**kwargs)\n",
        "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
        " \n",
        "    def call(self, x, sublayer_x):\n",
        "        # The sublayer input and output need to be of the same shape to be summed\n",
        "        add = x + sublayer_x\n",
        " \n",
        "        # Apply layer normalization to the sum\n",
        "        return self.layer_norm(add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv2vHc2sws0e"
      },
      "outputs": [],
      "source": [
        "# Implementing the Feed-Forward Layer\n",
        "class FeedForward(Layer):\n",
        "    def __init__(self, d_ff, d_model, **kwargs):\n",
        "        super(FeedForward, self).__init__(**kwargs)\n",
        "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
        "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
        "        self.activation = ReLU()  # ReLU activation layer\n",
        " \n",
        "    def call(self, x):\n",
        "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
        "        x_fc1 = self.fully_connected1(x)\n",
        " \n",
        "        return self.fully_connected2(self.activation(x_fc1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMpFWMK5wxEM"
      },
      "outputs": [],
      "source": [
        "# Implementing the Encoder Layer\n",
        "class EncoderLayer(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "        super(EncoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.add_norm1 = AddNormalization()\n",
        "        self.feed_forward = FeedForward(d_ff, d_model)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.add_norm2 = AddNormalization()\n",
        " \n",
        "    def call(self, x, padding_mask, training):\n",
        "        # Multi-head attention layer\n",
        "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Add in a dropout layer\n",
        "        multihead_output = self.dropout1(multihead_output, training=training)\n",
        " \n",
        "        # Followed by an Add & Norm layer\n",
        "        addnorm_output = self.add_norm1(x, multihead_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Followed by a fully connected layer\n",
        "        feedforward_output = self.feed_forward(addnorm_output)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Add in another dropout layer\n",
        "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
        " \n",
        "        # Followed by another Add & Norm layer\n",
        "        return self.add_norm2(addnorm_output, feedforward_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1O0lOcuw1st"
      },
      "outputs": [],
      "source": [
        "# Implementing the Encoder\n",
        "class Encoder(Layer):\n",
        "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "        super(Encoder, self).__init__(**kwargs)\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.dropout = Dropout(rate)\n",
        "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        " \n",
        "    def call(self, input_sentence, padding_mask, training):\n",
        "        # Generate the positional encoding\n",
        "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Add in a dropout layer\n",
        "        x = self.dropout(pos_encoding_output, training=training)\n",
        " \n",
        "        # Pass on the positional encoded values to each encoder layer\n",
        "        for i, layer in enumerate(self.encoder_layer):\n",
        "            x = layer(x, padding_mask, training)\n",
        " \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-ZtRy0ew7CL"
      },
      "outputs": [],
      "source": [
        "# Implementing the Decoder Layer\n",
        "class DecoderLayer(Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
        "        super(DecoderLayer, self).__init__(**kwargs)\n",
        "        self.multihead_attention1 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.add_norm1 = AddNormalization()\n",
        "        self.multihead_attention2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.add_norm2 = AddNormalization()\n",
        "        self.feed_forward = FeedForward(d_ff, d_model)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "        self.add_norm3 = AddNormalization()\n",
        "\n",
        "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
        "        # Multi-head attention layer\n",
        "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Add in a dropout layer\n",
        "        multihead_output1 = self.dropout1(multihead_output1, training=training)\n",
        " \n",
        "        # Followed by an Add & Norm layer\n",
        "        addnorm_output1 = self.add_norm1(x, multihead_output1)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Followed by another multi-head attention layer\n",
        "        multihead_output2 = self.multihead_attention2(addnorm_output1, encoder_output, encoder_output, padding_mask)\n",
        " \n",
        "        # Add in another dropout layer\n",
        "        multihead_output2 = self.dropout2(multihead_output2, training=training)\n",
        " \n",
        "        # Followed by another Add & Norm layer\n",
        "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)\n",
        " \n",
        "        # Followed by a fully connected layer\n",
        "        feedforward_output = self.feed_forward(addnorm_output2)\n",
        "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
        " \n",
        "        # Add in another dropout layer\n",
        "        feedforward_output = self.dropout3(feedforward_output, training=training)\n",
        " \n",
        "        # Followed by another Add & Norm layer\n",
        "        return self.add_norm3(addnorm_output2, feedforward_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mrALMe6xga8"
      },
      "outputs": [],
      "source": [
        "# Implementing the Decoder\n",
        "class Decoder(Layer):\n",
        "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
        "        self.dropout = Dropout(rate)\n",
        "        self.decoder_layer = [DecoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
        " \n",
        "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
        "        # Generate the positional encoding\n",
        "        pos_encoding_output = self.pos_encoding(output_target)\n",
        "        # Expected output shape = (number of sentences, sequence_length, d_model)\n",
        " \n",
        "        # Add in a dropout layer\n",
        "        x = self.dropout(pos_encoding_output, training=training)\n",
        " \n",
        "        # Pass on the positional encoded values to each encoder layer\n",
        "        for i, layer in enumerate(self.decoder_layer):\n",
        "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)\n",
        " \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H78lm2mxmwb"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(Model):\n",
        "    def __init__(self, enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate, **kwargs):\n",
        "        super(TransformerModel, self).__init__(**kwargs)\n",
        " \n",
        "        # Set up the encoder\n",
        "        self.encoder = Encoder(enc_vocab_size, enc_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
        " \n",
        "        # Set up the decoder\n",
        "        self.decoder = Decoder(dec_vocab_size, dec_seq_length, h, d_k, d_v, d_model, d_ff_inner, n, rate)\n",
        " \n",
        "        # Define the final dense layer\n",
        "        self.model_last_layer = Dense(dec_vocab_size)\n",
        " \n",
        "    def padding_mask(self, input):\n",
        "        # Create mask which marks the zero padding values in the input by a 1.0\n",
        "        mask = math.equal(input, 0)\n",
        "        mask = cast(mask, float32)\n",
        " \n",
        "        # The shape of the mask should be broadcastable to the shape\n",
        "        # of the attention weights that it will be masking later on\n",
        "        return mask[:, newaxis, newaxis, :]\n",
        " \n",
        "    def lookahead_mask(self, shape):\n",
        "        # Mask out future entries by marking them with a 1.0\n",
        "        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)\n",
        " \n",
        "        return mask\n",
        "\n",
        "    def call(self, encoder_input, decoder_input, training):\n",
        " \n",
        "        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n",
        "        enc_padding_mask = self.padding_mask(encoder_input)\n",
        " \n",
        "        # Create and combine padding and look-ahead masks to be fed into the decoder\n",
        "        dec_in_padding_mask = self.padding_mask(decoder_input)\n",
        "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])\n",
        "        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask)\n",
        " \n",
        "        # Feed the input into the encoder\n",
        "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
        " \n",
        "        # Feed the encoder output into the decoder\n",
        "        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training)\n",
        " \n",
        "        # Pass the decoder output through a final dense layer\n",
        "        model_output = self.model_last_layer(decoder_output)\n",
        " \n",
        "        return model_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf5iDVVwyLKZ"
      },
      "outputs": [],
      "source": [
        "class PrepareDataset:\n",
        "    def __init__(self, **kwargs):\n",
        "        super(PrepareDataset, self).__init__(**kwargs)\n",
        "        self.n_sentences = 10000  # Number of sentences to include in the dataset\n",
        "        self.train_split = 0.9  # Ratio of the training data split\n",
        "    \n",
        "    # Fit a tokenizer\n",
        "    def create_tokenizer(self, dataset):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(dataset)\n",
        "    \n",
        "        return tokenizer\n",
        "    \n",
        "    def find_seq_length(self, dataset):\n",
        "        return max(len(seq.split()) for seq in dataset)\n",
        "    \n",
        "    def find_vocab_size(self, tokenizer, dataset):\n",
        "        tokenizer.fit_on_texts(dataset)\n",
        "    \n",
        "        return len(tokenizer.word_index) + 1\n",
        "    \n",
        "    def __call__(self, filename, **kwargs):\n",
        "        # Load a clean dataset\n",
        "        #clean_dataset = load(open(filename, 'rb'))\n",
        "        \n",
        "        clean_dataset = pd.read_csv(\"./Hindi_English_Truncated_Corpus.csv\")\n",
        "        clean_dataset = clean_dataset.iloc[:,:].values\n",
        "        print(clean_dataset)\n",
        "    \n",
        "        # Reduce dataset size\n",
        "        dataset = clean_dataset[:self.n_sentences, :]\n",
        "\n",
        "        \n",
        "    \n",
        "        # Include start and end of string tokens\n",
        "        for i in range(dataset[:, 0].size):\n",
        "            dataset[i, 0] = \"<START> \" + dataset[i, 0] + \" <EOS>\"\n",
        "            dataset[i, 1] = \"<START> \" + dataset[i, 1] + \" <EOS>\"\n",
        "        \n",
        "        # Random shuffle the dataset\n",
        "        shuffle(dataset)\n",
        "        \n",
        "        # Split the dataset\n",
        "        train = dataset[:int(self.n_sentences * self.train_split)]\n",
        "        \n",
        "        # Prepare tokenizer for the encoder input\n",
        "        enc_tokenizer = self.create_tokenizer(train[:, 0])\n",
        "        enc_seq_length = self.find_seq_length(train[:, 0])\n",
        "        enc_vocab_size = self.find_vocab_size(enc_tokenizer, train[:, 0])\n",
        "        \n",
        "        # Encode and pad the input sequences\n",
        "        trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
        "        trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
        "        trainX = convert_to_tensor(trainX, dtype=int64)\n",
        "        \n",
        "        # Prepare tokenizer for the decoder input\n",
        "        dec_tokenizer = self.create_tokenizer(train[:, 1])\n",
        "        dec_seq_length = self.find_seq_length(train[:, 1])\n",
        "        dec_vocab_size = self.find_vocab_size(dec_tokenizer, train[:, 1])\n",
        "        \n",
        "        # Encode and pad the input sequences\n",
        "        trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
        "        trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
        "        trainY = convert_to_tensor(trainY, dtype=int64)\n",
        "        \n",
        "        return trainX, trainY, train, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBHqGMvfzMK_"
      },
      "outputs": [],
      "source": [
        "# Define the model parameters\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_model = 512  # Dimensionality of model layers' outputs\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
        "n = 6  # Number of layers in the encoder stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1vfx-7t0NAu"
      },
      "outputs": [],
      "source": [
        "# Define the training parameters\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.98\n",
        "epsilon = 1e-9\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0THBc3Ig0UyG"
      },
      "outputs": [],
      "source": [
        "# Implementing a learning rate scheduler\n",
        "class LRScheduler(LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
        "        super(LRScheduler, self).__init__(**kwargs)\n",
        "\n",
        "        self.d_model = cast(d_model, float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step_num):\n",
        "\n",
        "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
        "        arg1 = tf.cast(step_num, dtype=tf.float32) ** -0.5\n",
        "\n",
        "\n",
        "        arg2 = tf.cast(step_num, dtype=tf.float32) * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09pw3hdE0ZmT"
      },
      "outputs": [],
      "source": [
        "# Instantiate an Adam optimizer\n",
        "optimizer = Adam(LRScheduler(d_model), beta_1, beta_2, epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqNbnJMS3oIb",
        "outputId": "1ad5cb13-434b-453f-dc3d-fa6f8adf58b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ted' 'politicians do not have permission to do what needs to be done.'\n",
            "  'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .']\n",
            " ['ted' \"I'd like to tell you about one such child,\"\n",
            "  'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,']\n",
            " ['indic2012'\n",
            "  'This percentage is even greater than the percentage in India.'\n",
            "  'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।']\n",
            " ...\n",
            " ['tides'\n",
            "  'Bhaiji also says that the easiest way of getting such Swarajya is for all of us to give up our religion and adopt Christianity in which case we will become English and we shall become free . . ..'\n",
            "  \"यह भी उन्होंने दिखाया है कि इस प्रकार की नयी जातीयता और ' स्वराज़्य ' लेने का सबसे आसान तरीका यह है कि हम सब अपना धर्म छोड़कर ईसाई हो जायें- “ हमारा ' स्व ' इंग़्लैंड के लोगों का ' सेल्फ ' हो जायेगा और हम स्वतंत्र हो जायेंगे . ”\"]\n",
            " ['indic2012'\n",
            "  \"In this century the spelling of 'Mombaien' changed (1525) and it became 'Mombaim (1563) and ultimately in 16th century 'Bombaim' surfaced. as has been written by Gaspar Correia in Lendas da Índia (“”Legends of India“”).\"\n",
            "  'इसी शताब्दी में मोम्बाइयेन की वर्तनी बदली (१५२५) और वह मोंबैएम बना (१५६३) और अन्ततः सोलहवीं शताब्दी में बोम्बैएम उभरा जैसा गैस्पर कोर्रेइया ने लेंडास द इंडिया (लीजेंड्स ऑफ इंडिया) में लिखा है ।']\n",
            " ['tides'\n",
            "  'The central government or a state government is empowered to constitute -LRB- by notification in the Official Gazette -RRB- one or more designated courts for such area or areas or class or group of cases as may be specified in the notificat'\n",
            "  nan]]\n"
          ]
        }
      ],
      "source": [
        "# Prepare the training and test splits of the dataset\n",
        "dataset = PrepareDataset()\n",
        "trainX, trainY, train_orig, enc_seq_length, dec_seq_length, enc_vocab_size, dec_vocab_size = dataset('english-german-both.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dhod03z3sGk"
      },
      "outputs": [],
      "source": [
        "# Prepare the dataset batches\n",
        "train_dataset = data.Dataset.from_tensor_slices((trainX, trainY))\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOBXpWIg3wdb"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t68Lh-rs39Mm"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function\n",
        "def loss_fcn(target, prediction):\n",
        "    # Create mask so that the zero padding values are not included in the computation of loss\n",
        "    padding_mask = math.logical_not(equal(target, 0))\n",
        "    padding_mask = cast(padding_mask, float32)\n",
        "\n",
        "    # Compute a sparse categorical cross-entropy loss on the unmasked values\n",
        "    loss = sparse_categorical_crossentropy(target, prediction, from_logits=True) * padding_mask\n",
        "\n",
        "    # Compute the mean loss over the unmasked values\n",
        "    return reduce_sum(loss) / reduce_sum(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLrsCUR84AUX"
      },
      "outputs": [],
      "source": [
        "# Defining the accuracy function\n",
        "def accuracy_fcn(target, prediction):\n",
        "    # Create mask so that the zero padding values are not included in the computation of accuracy\n",
        "    padding_mask = math.logical_not(equal(target, 0))\n",
        "\n",
        "    # Find equal prediction and target values, and apply the padding mask\n",
        "    accuracy = equal(target, argmax(prediction, axis=2))\n",
        "    accuracy = math.logical_and(padding_mask, accuracy)\n",
        "\n",
        "    # Cast the True/False values to 32-bit-precision floating-point numbers\n",
        "    padding_mask = cast(padding_mask, float32)\n",
        "    accuracy = cast(accuracy, float32)\n",
        "\n",
        "    # Compute the mean accuracy over the unmasked values\n",
        "    return reduce_sum(accuracy) / reduce_sum(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_-EDcev4DES"
      },
      "outputs": [],
      "source": [
        "# Include metrics monitoring\n",
        "train_loss = Mean(name='train_loss')\n",
        "train_accuracy = Mean(name='train_accuracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyo1nyCY4F1D"
      },
      "outputs": [],
      "source": [
        "# Create a checkpoint object and manager to manage multiple checkpoints\n",
        "ckpt = train.Checkpoint(model=training_model, optimizer=optimizer)\n",
        "ckpt_manager = train.CheckpointManager(ckpt, \"./checkpoints\", max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwlU_lFT4KzB",
        "outputId": "b53fd4e1-c7be-45f9-e26a-de8cc92da152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 1\n",
            "Epoch 1 Step 0 Loss 9.7862 Accuracy 0.0000\n",
            "Epoch 1 Step 50 Loss 9.6439 Accuracy 0.0313\n",
            "Epoch 1 Step 100 Loss 9.4213 Accuracy 0.0480\n",
            "Epoch 1 Step 150 Loss 9.2155 Accuracy 0.0541\n",
            "Epoch 1 Step 200 Loss 8.9944 Accuracy 0.0570\n",
            "Epoch 1 Step 250 Loss 8.7596 Accuracy 0.0601\n",
            "Epoch 1: Training Loss 8.6115, Training Accuracy 0.0625\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0 Loss 7.3186 Accuracy 0.0985\n",
            "Epoch 2 Step 50 Loss 7.1272 Accuracy 0.0949\n",
            "Epoch 2 Step 100 Loss 7.0461 Accuracy 0.0978\n",
            "Epoch 2 Step 150 Loss 7.0096 Accuracy 0.0981\n",
            "Epoch 2 Step 200 Loss 6.9754 Accuracy 0.0989\n",
            "Epoch 2 Step 250 Loss 6.9570 Accuracy 0.1000\n",
            "Epoch 2: Training Loss 6.9435, Training Accuracy 0.1005\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Step 0 Loss 6.9652 Accuracy 0.0985\n",
            "Epoch 3 Step 50 Loss 6.8216 Accuracy 0.1065\n",
            "Epoch 3 Step 100 Loss 6.7843 Accuracy 0.1070\n",
            "Epoch 3 Step 150 Loss 6.7636 Accuracy 0.1073\n",
            "Epoch 3 Step 200 Loss 6.7353 Accuracy 0.1088\n",
            "Epoch 3 Step 250 Loss 6.7182 Accuracy 0.1101\n",
            "Epoch 3: Training Loss 6.7053, Training Accuracy 0.1104\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Step 0 Loss 6.7373 Accuracy 0.1097\n",
            "Epoch 4 Step 50 Loss 6.5878 Accuracy 0.1149\n",
            "Epoch 4 Step 100 Loss 6.5485 Accuracy 0.1159\n",
            "Epoch 4 Step 150 Loss 6.5269 Accuracy 0.1161\n",
            "Epoch 4 Step 200 Loss 6.5020 Accuracy 0.1170\n",
            "Epoch 4 Step 250 Loss 6.4858 Accuracy 0.1176\n",
            "Epoch 4: Training Loss 6.4749, Training Accuracy 0.1176\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Step 0 Loss 6.5509 Accuracy 0.1041\n",
            "Epoch 5 Step 50 Loss 6.3896 Accuracy 0.1162\n",
            "Epoch 5 Step 100 Loss 6.3497 Accuracy 0.1187\n",
            "Epoch 5 Step 150 Loss 6.3308 Accuracy 0.1188\n",
            "Epoch 5 Step 200 Loss 6.3108 Accuracy 0.1194\n",
            "Epoch 5 Step 250 Loss 6.2958 Accuracy 0.1200\n",
            "Epoch 5: Training Loss 6.2862, Training Accuracy 0.1201\n",
            "Saved checkpoint at epoch 5\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Step 0 Loss 6.3591 Accuracy 0.1190\n",
            "Epoch 6 Step 50 Loss 6.2161 Accuracy 0.1228\n",
            "Epoch 6 Step 100 Loss 6.1831 Accuracy 0.1234\n",
            "Epoch 6 Step 150 Loss 6.1623 Accuracy 0.1226\n",
            "Epoch 6 Step 200 Loss 6.1442 Accuracy 0.1233\n",
            "Epoch 6 Step 250 Loss 6.1290 Accuracy 0.1239\n",
            "Epoch 6: Training Loss 6.1212, Training Accuracy 0.1242\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Step 0 Loss 6.1925 Accuracy 0.1190\n",
            "Epoch 7 Step 50 Loss 6.0584 Accuracy 0.1262\n",
            "Epoch 7 Step 100 Loss 6.0224 Accuracy 0.1269\n",
            "Epoch 7 Step 150 Loss 6.0051 Accuracy 0.1266\n",
            "Epoch 7 Step 200 Loss 5.9932 Accuracy 0.1271\n",
            "Epoch 7 Step 250 Loss 5.9787 Accuracy 0.1275\n",
            "Epoch 7: Training Loss 5.9709, Training Accuracy 0.1279\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Step 0 Loss 6.0631 Accuracy 0.1152\n",
            "Epoch 8 Step 50 Loss 5.9310 Accuracy 0.1304\n",
            "Epoch 8 Step 100 Loss 5.8979 Accuracy 0.1311\n",
            "Epoch 8 Step 150 Loss 5.9138 Accuracy 0.1275\n",
            "Epoch 8 Step 200 Loss 5.9100 Accuracy 0.1263\n",
            "Epoch 8 Step 250 Loss 5.9010 Accuracy 0.1255\n",
            "Epoch 8: Training Loss 5.8959, Training Accuracy 0.1255\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Step 0 Loss 6.0847 Accuracy 0.1115\n",
            "Epoch 9 Step 50 Loss 5.8780 Accuracy 0.1245\n",
            "Epoch 9 Step 100 Loss 5.8336 Accuracy 0.1261\n",
            "Epoch 9 Step 150 Loss 5.8192 Accuracy 0.1262\n",
            "Epoch 9 Step 200 Loss 5.8081 Accuracy 0.1265\n",
            "Epoch 9 Step 250 Loss 5.7976 Accuracy 0.1263\n",
            "Epoch 9: Training Loss 5.7984, Training Accuracy 0.1266\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Step 0 Loss 5.9899 Accuracy 0.1245\n",
            "Epoch 10 Step 50 Loss 5.8471 Accuracy 0.1254\n",
            "Epoch 10 Step 100 Loss 5.8101 Accuracy 0.1285\n",
            "Epoch 10 Step 150 Loss 5.8018 Accuracy 0.1280\n",
            "Epoch 10 Step 200 Loss 5.7942 Accuracy 0.1281\n",
            "Epoch 10 Step 250 Loss 5.7852 Accuracy 0.1284\n",
            "Epoch 10: Training Loss 5.7773, Training Accuracy 0.1289\n",
            "Saved checkpoint at epoch 10\n",
            "\n",
            "Start of epoch 11\n",
            "Epoch 11 Step 0 Loss 5.9204 Accuracy 0.1171\n",
            "Epoch 11 Step 50 Loss 5.7535 Accuracy 0.1282\n",
            "Epoch 11 Step 100 Loss 5.7203 Accuracy 0.1306\n",
            "Epoch 11 Step 150 Loss 5.7062 Accuracy 0.1304\n",
            "Epoch 11 Step 200 Loss 5.6959 Accuracy 0.1306\n",
            "Epoch 11 Step 250 Loss 5.6864 Accuracy 0.1304\n",
            "Epoch 11: Training Loss 5.6773, Training Accuracy 0.1308\n",
            "\n",
            "Start of epoch 12\n",
            "Epoch 12 Step 0 Loss 5.7823 Accuracy 0.1171\n",
            "Epoch 12 Step 50 Loss 5.6653 Accuracy 0.1296\n",
            "Epoch 12 Step 100 Loss 5.6157 Accuracy 0.1340\n",
            "Epoch 12 Step 150 Loss 5.6007 Accuracy 0.1331\n",
            "Epoch 12 Step 200 Loss 5.5951 Accuracy 0.1335\n",
            "Epoch 12 Step 250 Loss 5.5820 Accuracy 0.1336\n",
            "Epoch 12: Training Loss 5.5711, Training Accuracy 0.1339\n",
            "\n",
            "Start of epoch 13\n",
            "Epoch 13 Step 0 Loss 5.6304 Accuracy 0.1338\n",
            "Epoch 13 Step 50 Loss 5.5544 Accuracy 0.1336\n",
            "Epoch 13 Step 100 Loss 5.5218 Accuracy 0.1361\n",
            "Epoch 13 Step 150 Loss 5.4957 Accuracy 0.1357\n",
            "Epoch 13 Step 200 Loss 5.4798 Accuracy 0.1366\n",
            "Epoch 13 Step 250 Loss 5.4724 Accuracy 0.1362\n",
            "Epoch 13: Training Loss 5.4656, Training Accuracy 0.1362\n",
            "\n",
            "Start of epoch 14\n",
            "Epoch 14 Step 0 Loss 5.5361 Accuracy 0.1227\n",
            "Epoch 14 Step 50 Loss 5.4527 Accuracy 0.1361\n",
            "Epoch 14 Step 100 Loss 5.4337 Accuracy 0.1370\n",
            "Epoch 14 Step 150 Loss 5.3960 Accuracy 0.1376\n",
            "Epoch 14 Step 200 Loss 5.3771 Accuracy 0.1383\n",
            "Epoch 14 Step 250 Loss 5.3682 Accuracy 0.1379\n",
            "Epoch 14: Training Loss 5.3606, Training Accuracy 0.1381\n",
            "\n",
            "Start of epoch 15\n",
            "Epoch 15 Step 0 Loss 5.5279 Accuracy 0.1357\n",
            "Epoch 15 Step 50 Loss 5.3459 Accuracy 0.1385\n",
            "Epoch 15 Step 100 Loss 5.3113 Accuracy 0.1397\n",
            "Epoch 15 Step 150 Loss 5.2779 Accuracy 0.1409\n",
            "Epoch 15 Step 200 Loss 5.2548 Accuracy 0.1419\n",
            "Epoch 15 Step 250 Loss 5.2396 Accuracy 0.1416\n",
            "Epoch 15: Training Loss 5.2296, Training Accuracy 0.1422\n",
            "Saved checkpoint at epoch 15\n",
            "\n",
            "Start of epoch 16\n",
            "Epoch 16 Step 0 Loss 5.4501 Accuracy 0.1078\n",
            "Epoch 16 Step 50 Loss 5.1613 Accuracy 0.1429\n",
            "Epoch 16 Step 100 Loss 5.1388 Accuracy 0.1439\n",
            "Epoch 16 Step 150 Loss 5.1194 Accuracy 0.1442\n",
            "Epoch 16 Step 200 Loss 5.0979 Accuracy 0.1455\n",
            "Epoch 16 Step 250 Loss 5.0803 Accuracy 0.1455\n",
            "Epoch 16: Training Loss 5.0702, Training Accuracy 0.1461\n",
            "\n",
            "Start of epoch 17\n",
            "Epoch 17 Step 0 Loss 5.1866 Accuracy 0.1431\n",
            "Epoch 17 Step 50 Loss 5.0016 Accuracy 0.1498\n",
            "Epoch 17 Step 100 Loss 4.9848 Accuracy 0.1506\n",
            "Epoch 17 Step 150 Loss 4.9622 Accuracy 0.1510\n",
            "Epoch 17 Step 200 Loss 4.9487 Accuracy 0.1516\n",
            "Epoch 17 Step 250 Loss 4.9264 Accuracy 0.1514\n",
            "Epoch 17: Training Loss 4.9090, Training Accuracy 0.1529\n",
            "\n",
            "Start of epoch 18\n",
            "Epoch 18 Step 0 Loss 4.9757 Accuracy 0.1580\n",
            "Epoch 18 Step 50 Loss 4.7942 Accuracy 0.1593\n",
            "Epoch 18 Step 100 Loss 4.8171 Accuracy 0.1585\n",
            "Epoch 18 Step 150 Loss 4.7956 Accuracy 0.1585\n",
            "Epoch 18 Step 200 Loss 4.7928 Accuracy 0.1583\n",
            "Epoch 18 Step 250 Loss 4.7708 Accuracy 0.1586\n",
            "Epoch 18: Training Loss 4.7538, Training Accuracy 0.1600\n",
            "\n",
            "Start of epoch 19\n",
            "Epoch 19 Step 0 Loss 4.9410 Accuracy 0.1431\n",
            "Epoch 19 Step 50 Loss 4.6774 Accuracy 0.1647\n",
            "Epoch 19 Step 100 Loss 4.6830 Accuracy 0.1647\n",
            "Epoch 19 Step 150 Loss 4.6794 Accuracy 0.1652\n",
            "Epoch 19 Step 200 Loss 4.6731 Accuracy 0.1655\n",
            "Epoch 19 Step 250 Loss 4.6501 Accuracy 0.1660\n",
            "Epoch 19: Training Loss 4.6293, Training Accuracy 0.1683\n",
            "\n",
            "Start of epoch 20\n",
            "Epoch 20 Step 0 Loss 4.6469 Accuracy 0.1654\n",
            "Epoch 20 Step 50 Loss 4.5850 Accuracy 0.1721\n",
            "Epoch 20 Step 100 Loss 4.5740 Accuracy 0.1724\n",
            "Epoch 20 Step 150 Loss 4.5579 Accuracy 0.1745\n",
            "Epoch 20 Step 200 Loss 4.5449 Accuracy 0.1748\n",
            "Epoch 20 Step 250 Loss 4.5108 Accuracy 0.1762\n",
            "Epoch 20: Training Loss 4.4904, Training Accuracy 0.1785\n",
            "Saved checkpoint at epoch 20\n",
            "\n",
            "Start of epoch 21\n",
            "Epoch 21 Step 0 Loss 4.4331 Accuracy 0.1691\n",
            "Epoch 21 Step 50 Loss 4.3716 Accuracy 0.1904\n",
            "Epoch 21 Step 100 Loss 4.3802 Accuracy 0.1888\n",
            "Epoch 21 Step 150 Loss 4.3713 Accuracy 0.1890\n",
            "Epoch 21 Step 200 Loss 4.3607 Accuracy 0.1893\n",
            "Epoch 21 Step 250 Loss 4.3339 Accuracy 0.1912\n",
            "Epoch 21: Training Loss 4.3319, Training Accuracy 0.1923\n",
            "\n",
            "Start of epoch 22\n",
            "Epoch 22 Step 0 Loss 4.2747 Accuracy 0.1877\n",
            "Epoch 22 Step 50 Loss 4.2843 Accuracy 0.1953\n",
            "Epoch 22 Step 100 Loss 4.2772 Accuracy 0.1960\n",
            "Epoch 22 Step 150 Loss 4.2524 Accuracy 0.1986\n",
            "Epoch 22 Step 200 Loss 4.2355 Accuracy 0.1998\n",
            "Epoch 22 Step 250 Loss 4.2000 Accuracy 0.2028\n",
            "Epoch 22: Training Loss 4.1894, Training Accuracy 0.2049\n",
            "\n",
            "Start of epoch 23\n",
            "Epoch 23 Step 0 Loss 4.2556 Accuracy 0.2026\n",
            "Epoch 23 Step 50 Loss 4.1122 Accuracy 0.2156\n",
            "Epoch 23 Step 100 Loss 4.1205 Accuracy 0.2133\n",
            "Epoch 23 Step 150 Loss 4.0985 Accuracy 0.2152\n",
            "Epoch 23 Step 200 Loss 4.0855 Accuracy 0.2162\n",
            "Epoch 23 Step 250 Loss 4.0559 Accuracy 0.2192\n",
            "Epoch 23: Training Loss 4.0393, Training Accuracy 0.2221\n",
            "\n",
            "Start of epoch 24\n",
            "Epoch 24 Step 0 Loss 4.1994 Accuracy 0.1914\n",
            "Epoch 24 Step 50 Loss 3.9708 Accuracy 0.2331\n",
            "Epoch 24 Step 100 Loss 3.9706 Accuracy 0.2322\n",
            "Epoch 24 Step 150 Loss 3.9368 Accuracy 0.2364\n",
            "Epoch 24 Step 200 Loss 3.9251 Accuracy 0.2364\n",
            "Epoch 24 Step 250 Loss 3.8930 Accuracy 0.2393\n",
            "Epoch 24: Training Loss 3.8834, Training Accuracy 0.2412\n",
            "\n",
            "Start of epoch 25\n",
            "Epoch 25 Step 0 Loss 4.0667 Accuracy 0.2082\n",
            "Epoch 25 Step 50 Loss 3.8985 Accuracy 0.2420\n",
            "Epoch 25 Step 100 Loss 3.8945 Accuracy 0.2409\n",
            "Epoch 25 Step 150 Loss 3.8324 Accuracy 0.2488\n",
            "Epoch 25 Step 200 Loss 3.8035 Accuracy 0.2511\n",
            "Epoch 25 Step 250 Loss 3.7578 Accuracy 0.2560\n",
            "Epoch 25: Training Loss 3.7472, Training Accuracy 0.2579\n",
            "Saved checkpoint at epoch 25\n",
            "\n",
            "Start of epoch 26\n",
            "Epoch 26 Step 0 Loss 4.0956 Accuracy 0.2026\n",
            "Epoch 26 Step 50 Loss 3.7780 Accuracy 0.2512\n",
            "Epoch 26 Step 100 Loss 3.7614 Accuracy 0.2540\n",
            "Epoch 26 Step 150 Loss 3.6983 Accuracy 0.2629\n",
            "Epoch 26 Step 200 Loss 3.6625 Accuracy 0.2670\n",
            "Epoch 26 Step 250 Loss 3.6166 Accuracy 0.2721\n",
            "Epoch 26: Training Loss 3.5924, Training Accuracy 0.2766\n",
            "\n",
            "Start of epoch 27\n",
            "Epoch 27 Step 0 Loss 3.7569 Accuracy 0.2454\n",
            "Epoch 27 Step 50 Loss 3.5262 Accuracy 0.2867\n",
            "Epoch 27 Step 100 Loss 3.5412 Accuracy 0.2849\n",
            "Epoch 27 Step 150 Loss 3.5056 Accuracy 0.2904\n",
            "Epoch 27 Step 200 Loss 3.4850 Accuracy 0.2923\n",
            "Epoch 27 Step 250 Loss 3.4451 Accuracy 0.2977\n",
            "Epoch 27: Training Loss 3.4328, Training Accuracy 0.3000\n",
            "\n",
            "Start of epoch 28\n",
            "Epoch 28 Step 0 Loss 3.5995 Accuracy 0.2695\n",
            "Epoch 28 Step 50 Loss 3.4559 Accuracy 0.2974\n",
            "Epoch 28 Step 100 Loss 3.4502 Accuracy 0.2981\n",
            "Epoch 28 Step 150 Loss 3.3974 Accuracy 0.3047\n",
            "Epoch 28 Step 200 Loss 3.3762 Accuracy 0.3069\n",
            "Epoch 28 Step 250 Loss 3.3387 Accuracy 0.3118\n",
            "Epoch 28: Training Loss 3.3245, Training Accuracy 0.3140\n",
            "\n",
            "Start of epoch 29\n",
            "Epoch 29 Step 0 Loss 3.4097 Accuracy 0.2937\n",
            "Epoch 29 Step 50 Loss 3.2543 Accuracy 0.3243\n",
            "Epoch 29 Step 100 Loss 3.2622 Accuracy 0.3224\n",
            "Epoch 29 Step 150 Loss 3.2105 Accuracy 0.3309\n",
            "Epoch 29 Step 200 Loss 3.2044 Accuracy 0.3303\n",
            "Epoch 29 Step 250 Loss 3.1739 Accuracy 0.3352\n",
            "Epoch 29: Training Loss 3.1489, Training Accuracy 0.3397\n",
            "\n",
            "Start of epoch 30\n",
            "Epoch 30 Step 0 Loss 3.1873 Accuracy 0.3439\n",
            "Epoch 30 Step 50 Loss 3.1249 Accuracy 0.3451\n",
            "Epoch 30 Step 100 Loss 3.1376 Accuracy 0.3420\n",
            "Epoch 30 Step 150 Loss 3.0876 Accuracy 0.3499\n",
            "Epoch 30 Step 200 Loss 3.0745 Accuracy 0.3505\n",
            "Epoch 30 Step 250 Loss 3.0526 Accuracy 0.3534\n",
            "Epoch 30: Training Loss 3.0240, Training Accuracy 0.3588\n",
            "Saved checkpoint at epoch 30\n",
            "\n",
            "Start of epoch 31\n",
            "Epoch 31 Step 0 Loss 3.0721 Accuracy 0.3401\n",
            "Epoch 31 Step 50 Loss 2.9264 Accuracy 0.3759\n",
            "Epoch 31 Step 100 Loss 2.9431 Accuracy 0.3749\n",
            "Epoch 31 Step 150 Loss 2.9211 Accuracy 0.3779\n",
            "Epoch 31 Step 200 Loss 2.9079 Accuracy 0.3781\n",
            "Epoch 31 Step 250 Loss 2.9006 Accuracy 0.3780\n",
            "Epoch 31: Training Loss 2.8824, Training Accuracy 0.3814\n",
            "\n",
            "Start of epoch 32\n",
            "Epoch 32 Step 0 Loss 2.9511 Accuracy 0.3476\n",
            "Epoch 32 Step 50 Loss 2.8733 Accuracy 0.3811\n",
            "Epoch 32 Step 100 Loss 2.8776 Accuracy 0.3802\n",
            "Epoch 32 Step 150 Loss 2.8394 Accuracy 0.3867\n",
            "Epoch 32 Step 200 Loss 2.8156 Accuracy 0.3895\n",
            "Epoch 32 Step 250 Loss 2.7922 Accuracy 0.3931\n",
            "Epoch 32: Training Loss 2.7701, Training Accuracy 0.3974\n",
            "\n",
            "Start of epoch 33\n",
            "Epoch 33 Step 0 Loss 2.7784 Accuracy 0.4052\n",
            "Epoch 33 Step 50 Loss 2.6841 Accuracy 0.4146\n",
            "Epoch 33 Step 100 Loss 2.6947 Accuracy 0.4133\n",
            "Epoch 33 Step 150 Loss 2.6676 Accuracy 0.4177\n",
            "Epoch 33 Step 200 Loss 2.6517 Accuracy 0.4187\n",
            "Epoch 33 Step 250 Loss 2.6455 Accuracy 0.4190\n",
            "Epoch 33: Training Loss 2.6280, Training Accuracy 0.4221\n",
            "\n",
            "Start of epoch 34\n",
            "Epoch 34 Step 0 Loss 2.6449 Accuracy 0.4089\n",
            "Epoch 34 Step 50 Loss 2.6232 Accuracy 0.4219\n",
            "Epoch 34 Step 100 Loss 2.6196 Accuracy 0.4225\n",
            "Epoch 34 Step 150 Loss 2.5781 Accuracy 0.4305\n",
            "Epoch 34 Step 200 Loss 2.5710 Accuracy 0.4305\n",
            "Epoch 34 Step 250 Loss 2.5797 Accuracy 0.4289\n",
            "Epoch 34: Training Loss 2.5692, Training Accuracy 0.4308\n",
            "\n",
            "Start of epoch 35\n",
            "Epoch 35 Step 0 Loss 2.6583 Accuracy 0.4517\n",
            "Epoch 35 Step 50 Loss 2.5669 Accuracy 0.4315\n",
            "Epoch 35 Step 100 Loss 2.5571 Accuracy 0.4328\n",
            "Epoch 35 Step 150 Loss 2.5064 Accuracy 0.4417\n",
            "Epoch 35 Step 200 Loss 2.4975 Accuracy 0.4424\n",
            "Epoch 35 Step 250 Loss 2.5033 Accuracy 0.4412\n",
            "Epoch 35: Training Loss 2.4892, Training Accuracy 0.4443\n",
            "Saved checkpoint at epoch 35\n",
            "\n",
            "Start of epoch 36\n",
            "Epoch 36 Step 0 Loss 2.4207 Accuracy 0.4610\n",
            "Epoch 36 Step 50 Loss 2.4641 Accuracy 0.4507\n",
            "Epoch 36 Step 100 Loss 2.4263 Accuracy 0.4562\n",
            "Epoch 36 Step 150 Loss 2.3810 Accuracy 0.4655\n",
            "Epoch 36 Step 200 Loss 2.3737 Accuracy 0.4656\n",
            "Epoch 36 Step 250 Loss 2.3798 Accuracy 0.4646\n",
            "Epoch 36: Training Loss 2.3645, Training Accuracy 0.4680\n",
            "\n",
            "Start of epoch 37\n",
            "Epoch 37 Step 0 Loss 2.2792 Accuracy 0.4703\n",
            "Epoch 37 Step 50 Loss 2.3672 Accuracy 0.4665\n",
            "Epoch 37 Step 100 Loss 2.3499 Accuracy 0.4688\n",
            "Epoch 37 Step 150 Loss 2.3007 Accuracy 0.4782\n",
            "Epoch 37 Step 200 Loss 2.2818 Accuracy 0.4813\n",
            "Epoch 37 Step 250 Loss 2.2789 Accuracy 0.4824\n",
            "Epoch 37: Training Loss 2.2722, Training Accuracy 0.4838\n",
            "\n",
            "Start of epoch 38\n",
            "Epoch 38 Step 0 Loss 2.1739 Accuracy 0.5019\n",
            "Epoch 38 Step 50 Loss 2.2588 Accuracy 0.4841\n",
            "Epoch 38 Step 100 Loss 2.2538 Accuracy 0.4848\n",
            "Epoch 38 Step 150 Loss 2.2136 Accuracy 0.4932\n",
            "Epoch 38 Step 200 Loss 2.1927 Accuracy 0.4976\n",
            "Epoch 38 Step 250 Loss 2.1913 Accuracy 0.4974\n",
            "Epoch 38: Training Loss 2.1814, Training Accuracy 0.4995\n",
            "\n",
            "Start of epoch 39\n",
            "Epoch 39 Step 0 Loss 2.1301 Accuracy 0.4944\n",
            "Epoch 39 Step 50 Loss 2.1475 Accuracy 0.5073\n",
            "Epoch 39 Step 100 Loss 2.1227 Accuracy 0.5114\n",
            "Epoch 39 Step 150 Loss 2.0894 Accuracy 0.5178\n",
            "Epoch 39 Step 200 Loss 2.0749 Accuracy 0.5209\n",
            "Epoch 39 Step 250 Loss 2.0813 Accuracy 0.5191\n",
            "Epoch 39: Training Loss 2.0821, Training Accuracy 0.5189\n",
            "\n",
            "Start of epoch 40\n",
            "Epoch 40 Step 0 Loss 2.0359 Accuracy 0.5502\n",
            "Epoch 40 Step 50 Loss 2.0254 Accuracy 0.5307\n",
            "Epoch 40 Step 100 Loss 2.0176 Accuracy 0.5304\n",
            "Epoch 40 Step 150 Loss 1.9968 Accuracy 0.5357\n",
            "Epoch 40 Step 200 Loss 1.9801 Accuracy 0.5391\n",
            "Epoch 40 Step 250 Loss 1.9863 Accuracy 0.5372\n",
            "Epoch 40: Training Loss 1.9895, Training Accuracy 0.5371\n",
            "Saved checkpoint at epoch 40\n",
            "\n",
            "Start of epoch 41\n",
            "Epoch 41 Step 0 Loss 1.8786 Accuracy 0.5632\n",
            "Epoch 41 Step 50 Loss 1.9375 Accuracy 0.5489\n",
            "Epoch 41 Step 100 Loss 1.9255 Accuracy 0.5515\n",
            "Epoch 41 Step 150 Loss 1.9128 Accuracy 0.5535\n",
            "Epoch 41 Step 200 Loss 1.9143 Accuracy 0.5525\n",
            "Epoch 41 Step 250 Loss 1.9184 Accuracy 0.5511\n",
            "Epoch 41: Training Loss 1.9197, Training Accuracy 0.5512\n",
            "\n",
            "Start of epoch 42\n",
            "Epoch 42 Step 0 Loss 1.8560 Accuracy 0.5706\n",
            "Epoch 42 Step 50 Loss 1.8481 Accuracy 0.5702\n",
            "Epoch 42 Step 100 Loss 1.8493 Accuracy 0.5679\n",
            "Epoch 42 Step 150 Loss 1.8377 Accuracy 0.5696\n",
            "Epoch 42 Step 200 Loss 1.8459 Accuracy 0.5671\n",
            "Epoch 42 Step 250 Loss 1.8532 Accuracy 0.5661\n",
            "Epoch 42: Training Loss 1.8590, Training Accuracy 0.5650\n",
            "\n",
            "Start of epoch 43\n",
            "Epoch 43 Step 0 Loss 1.8078 Accuracy 0.5558\n",
            "Epoch 43 Step 50 Loss 1.8057 Accuracy 0.5752\n",
            "Epoch 43 Step 100 Loss 1.8007 Accuracy 0.5755\n",
            "Epoch 43 Step 150 Loss 1.7789 Accuracy 0.5802\n",
            "Epoch 43 Step 200 Loss 1.7708 Accuracy 0.5823\n",
            "Epoch 43 Step 250 Loss 1.7757 Accuracy 0.5814\n",
            "Epoch 43: Training Loss 1.7768, Training Accuracy 0.5811\n",
            "\n",
            "Start of epoch 44\n",
            "Epoch 44 Step 0 Loss 1.6706 Accuracy 0.6078\n",
            "Epoch 44 Step 50 Loss 1.6817 Accuracy 0.6041\n",
            "Epoch 44 Step 100 Loss 1.6675 Accuracy 0.6042\n",
            "Epoch 44 Step 150 Loss 1.6556 Accuracy 0.6062\n",
            "Epoch 44 Step 200 Loss 1.6620 Accuracy 0.6045\n",
            "Epoch 44 Step 250 Loss 1.6704 Accuracy 0.6030\n",
            "Epoch 44: Training Loss 1.6693, Training Accuracy 0.6036\n",
            "\n",
            "Start of epoch 45\n",
            "Epoch 45 Step 0 Loss 1.6266 Accuracy 0.6078\n",
            "Epoch 45 Step 50 Loss 1.6249 Accuracy 0.6138\n",
            "Epoch 45 Step 100 Loss 1.6038 Accuracy 0.6177\n",
            "Epoch 45 Step 150 Loss 1.5889 Accuracy 0.6220\n",
            "Epoch 45 Step 200 Loss 1.5911 Accuracy 0.6220\n",
            "Epoch 45 Step 250 Loss 1.5952 Accuracy 0.6206\n",
            "Epoch 45: Training Loss 1.5987, Training Accuracy 0.6195\n",
            "Saved checkpoint at epoch 45\n",
            "\n",
            "Start of epoch 46\n",
            "Epoch 46 Step 0 Loss 1.6012 Accuracy 0.6134\n",
            "Epoch 46 Step 50 Loss 1.5867 Accuracy 0.6201\n",
            "Epoch 46 Step 100 Loss 1.5577 Accuracy 0.6282\n",
            "Epoch 46 Step 150 Loss 1.5411 Accuracy 0.6319\n",
            "Epoch 46 Step 200 Loss 1.5263 Accuracy 0.6361\n",
            "Epoch 46 Step 250 Loss 1.5306 Accuracy 0.6350\n",
            "Epoch 46: Training Loss 1.5423, Training Accuracy 0.6330\n",
            "\n",
            "Start of epoch 47\n",
            "Epoch 47 Step 0 Loss 1.5535 Accuracy 0.6506\n",
            "Epoch 47 Step 50 Loss 1.5644 Accuracy 0.6262\n",
            "Epoch 47 Step 100 Loss 1.5327 Accuracy 0.6346\n",
            "Epoch 47 Step 150 Loss 1.5177 Accuracy 0.6383\n",
            "Epoch 47 Step 200 Loss 1.5112 Accuracy 0.6393\n",
            "Epoch 47 Step 250 Loss 1.5071 Accuracy 0.6403\n",
            "Epoch 47: Training Loss 1.5129, Training Accuracy 0.6393\n",
            "\n",
            "Start of epoch 48\n",
            "Epoch 48 Step 0 Loss 1.6837 Accuracy 0.5669\n",
            "Epoch 48 Step 50 Loss 1.5207 Accuracy 0.6364\n",
            "Epoch 48 Step 100 Loss 1.4907 Accuracy 0.6427\n",
            "Epoch 48 Step 150 Loss 1.4791 Accuracy 0.6460\n",
            "Epoch 48 Step 200 Loss 1.4746 Accuracy 0.6468\n",
            "Epoch 48 Step 250 Loss 1.4669 Accuracy 0.6491\n",
            "Epoch 48: Training Loss 1.4721, Training Accuracy 0.6483\n",
            "\n",
            "Start of epoch 49\n",
            "Epoch 49 Step 0 Loss 1.4758 Accuracy 0.6543\n",
            "Epoch 49 Step 50 Loss 1.5404 Accuracy 0.6313\n",
            "Epoch 49 Step 100 Loss 1.4988 Accuracy 0.6423\n",
            "Epoch 49 Step 150 Loss 1.4784 Accuracy 0.6463\n",
            "Epoch 49 Step 200 Loss 1.4567 Accuracy 0.6517\n",
            "Epoch 49 Step 250 Loss 1.4550 Accuracy 0.6522\n",
            "Epoch 49: Training Loss 1.4599, Training Accuracy 0.6519\n",
            "\n",
            "Start of epoch 50\n",
            "Epoch 50 Step 0 Loss 1.4180 Accuracy 0.6747\n",
            "Epoch 50 Step 50 Loss 1.4522 Accuracy 0.6535\n",
            "Epoch 50 Step 100 Loss 1.4268 Accuracy 0.6592\n",
            "Epoch 50 Step 150 Loss 1.4162 Accuracy 0.6603\n",
            "Epoch 50 Step 200 Loss 1.4026 Accuracy 0.6648\n",
            "Epoch 50 Step 250 Loss 1.4032 Accuracy 0.6645\n",
            "Epoch 50: Training Loss 1.4091, Training Accuracy 0.6636\n",
            "Saved checkpoint at epoch 50\n",
            "\n",
            "Start of epoch 51\n",
            "Epoch 51 Step 0 Loss 1.3865 Accuracy 0.6803\n",
            "Epoch 51 Step 50 Loss 1.4248 Accuracy 0.6586\n",
            "Epoch 51 Step 100 Loss 1.3797 Accuracy 0.6704\n",
            "Epoch 51 Step 150 Loss 1.3899 Accuracy 0.6673\n",
            "Epoch 51 Step 200 Loss 1.3715 Accuracy 0.6724\n",
            "Epoch 51 Step 250 Loss 1.3657 Accuracy 0.6738\n",
            "Epoch 51: Training Loss 1.3721, Training Accuracy 0.6726\n",
            "\n",
            "Start of epoch 52\n",
            "Epoch 52 Step 0 Loss 1.4747 Accuracy 0.6375\n",
            "Epoch 52 Step 50 Loss 1.3716 Accuracy 0.6742\n",
            "Epoch 52 Step 100 Loss 1.3282 Accuracy 0.6835\n",
            "Epoch 52 Step 150 Loss 1.3312 Accuracy 0.6825\n",
            "Epoch 52 Step 200 Loss 1.3186 Accuracy 0.6852\n",
            "Epoch 52 Step 250 Loss 1.3248 Accuracy 0.6844\n",
            "Epoch 52: Training Loss 1.3368, Training Accuracy 0.6817\n",
            "\n",
            "Start of epoch 53\n",
            "Epoch 53 Step 0 Loss 1.3692 Accuracy 0.6729\n",
            "Epoch 53 Step 50 Loss 1.3692 Accuracy 0.6718\n",
            "Epoch 53 Step 100 Loss 1.3265 Accuracy 0.6827\n",
            "Epoch 53 Step 150 Loss 1.3277 Accuracy 0.6828\n",
            "Epoch 53 Step 200 Loss 1.3093 Accuracy 0.6877\n",
            "Epoch 53 Step 250 Loss 1.3092 Accuracy 0.6886\n",
            "Epoch 53: Training Loss 1.3177, Training Accuracy 0.6872\n",
            "\n",
            "Start of epoch 54\n",
            "Epoch 54 Step 0 Loss 1.3129 Accuracy 0.6766\n",
            "Epoch 54 Step 50 Loss 1.3193 Accuracy 0.6839\n",
            "Epoch 54 Step 100 Loss 1.2804 Accuracy 0.6961\n",
            "Epoch 54 Step 150 Loss 1.2928 Accuracy 0.6929\n",
            "Epoch 54 Step 200 Loss 1.2792 Accuracy 0.6961\n",
            "Epoch 54 Step 250 Loss 1.2749 Accuracy 0.6976\n",
            "Epoch 54: Training Loss 1.2855, Training Accuracy 0.6955\n",
            "\n",
            "Start of epoch 55\n",
            "Epoch 55 Step 0 Loss 1.4053 Accuracy 0.6468\n",
            "Epoch 55 Step 50 Loss 1.3569 Accuracy 0.6783\n",
            "Epoch 55 Step 100 Loss 1.2826 Accuracy 0.6971\n",
            "Epoch 55 Step 150 Loss 1.2815 Accuracy 0.6960\n",
            "Epoch 55 Step 200 Loss 1.2653 Accuracy 0.6999\n",
            "Epoch 55 Step 250 Loss 1.2580 Accuracy 0.7019\n",
            "Epoch 55: Training Loss 1.2668, Training Accuracy 0.7000\n",
            "Saved checkpoint at epoch 55\n",
            "\n",
            "Start of epoch 56\n",
            "Epoch 56 Step 0 Loss 1.1997 Accuracy 0.7361\n",
            "Epoch 56 Step 50 Loss 1.3090 Accuracy 0.6888\n",
            "Epoch 56 Step 100 Loss 1.2612 Accuracy 0.7008\n",
            "Epoch 56 Step 150 Loss 1.2615 Accuracy 0.7000\n",
            "Epoch 56 Step 200 Loss 1.2408 Accuracy 0.7051\n",
            "Epoch 56 Step 250 Loss 1.2320 Accuracy 0.7075\n",
            "Epoch 56: Training Loss 1.2421, Training Accuracy 0.7055\n",
            "\n",
            "Start of epoch 57\n",
            "Epoch 57 Step 0 Loss 1.2353 Accuracy 0.7212\n",
            "Epoch 57 Step 50 Loss 1.2836 Accuracy 0.6964\n",
            "Epoch 57 Step 100 Loss 1.2304 Accuracy 0.7095\n",
            "Epoch 57 Step 150 Loss 1.2386 Accuracy 0.7069\n",
            "Epoch 57 Step 200 Loss 1.2207 Accuracy 0.7109\n",
            "Epoch 57 Step 250 Loss 1.2119 Accuracy 0.7136\n",
            "Epoch 57: Training Loss 1.2158, Training Accuracy 0.7125\n",
            "\n",
            "Start of epoch 58\n",
            "Epoch 58 Step 0 Loss 1.1990 Accuracy 0.7212\n",
            "Epoch 58 Step 50 Loss 1.2626 Accuracy 0.7030\n",
            "Epoch 58 Step 100 Loss 1.2101 Accuracy 0.7155\n",
            "Epoch 58 Step 150 Loss 1.2152 Accuracy 0.7133\n",
            "Epoch 58 Step 200 Loss 1.1952 Accuracy 0.7184\n",
            "Epoch 58 Step 250 Loss 1.1816 Accuracy 0.7222\n",
            "Epoch 58: Training Loss 1.1855, Training Accuracy 0.7219\n",
            "\n",
            "Start of epoch 59\n",
            "Epoch 59 Step 0 Loss 1.2028 Accuracy 0.7416\n",
            "Epoch 59 Step 50 Loss 1.2083 Accuracy 0.7171\n",
            "Epoch 59 Step 100 Loss 1.1575 Accuracy 0.7304\n",
            "Epoch 59 Step 150 Loss 1.1514 Accuracy 0.7312\n",
            "Epoch 59 Step 200 Loss 1.1365 Accuracy 0.7334\n",
            "Epoch 59 Step 250 Loss 1.1279 Accuracy 0.7362\n",
            "Epoch 59: Training Loss 1.1296, Training Accuracy 0.7359\n",
            "\n",
            "Start of epoch 60\n",
            "Epoch 60 Step 0 Loss 1.1763 Accuracy 0.7435\n",
            "Epoch 60 Step 50 Loss 1.1362 Accuracy 0.7356\n",
            "Epoch 60 Step 100 Loss 1.1079 Accuracy 0.7425\n",
            "Epoch 60 Step 150 Loss 1.1086 Accuracy 0.7427\n",
            "Epoch 60 Step 200 Loss 1.0921 Accuracy 0.7454\n",
            "Epoch 60 Step 250 Loss 1.0861 Accuracy 0.7471\n",
            "Epoch 60: Training Loss 1.0880, Training Accuracy 0.7468\n",
            "Saved checkpoint at epoch 60\n",
            "\n",
            "Start of epoch 61\n",
            "Epoch 61 Step 0 Loss 1.1007 Accuracy 0.7621\n",
            "Epoch 61 Step 50 Loss 1.1111 Accuracy 0.7416\n",
            "Epoch 61 Step 100 Loss 1.0780 Accuracy 0.7497\n",
            "Epoch 61 Step 150 Loss 1.0863 Accuracy 0.7468\n",
            "Epoch 61 Step 200 Loss 1.0699 Accuracy 0.7500\n",
            "Epoch 61 Step 250 Loss 1.0637 Accuracy 0.7520\n",
            "Epoch 61: Training Loss 1.0650, Training Accuracy 0.7520\n",
            "\n",
            "Start of epoch 62\n",
            "Epoch 62 Step 0 Loss 1.1309 Accuracy 0.7435\n",
            "Epoch 62 Step 50 Loss 1.0792 Accuracy 0.7491\n",
            "Epoch 62 Step 100 Loss 1.0475 Accuracy 0.7574\n",
            "Epoch 62 Step 150 Loss 1.0544 Accuracy 0.7557\n",
            "Epoch 62 Step 200 Loss 1.0478 Accuracy 0.7571\n",
            "Epoch 62 Step 250 Loss 1.0411 Accuracy 0.7588\n",
            "Epoch 62: Training Loss 1.0393, Training Accuracy 0.7593\n",
            "\n",
            "Start of epoch 63\n",
            "Epoch 63 Step 0 Loss 1.1163 Accuracy 0.7454\n",
            "Epoch 63 Step 50 Loss 1.0338 Accuracy 0.7603\n",
            "Epoch 63 Step 100 Loss 1.0111 Accuracy 0.7672\n",
            "Epoch 63 Step 150 Loss 1.0180 Accuracy 0.7652\n",
            "Epoch 63 Step 200 Loss 1.0072 Accuracy 0.7677\n",
            "Epoch 63 Step 250 Loss 1.0099 Accuracy 0.7676\n",
            "Epoch 63: Training Loss 1.0123, Training Accuracy 0.7673\n",
            "\n",
            "Start of epoch 64\n",
            "Epoch 64 Step 0 Loss 0.9915 Accuracy 0.7993\n",
            "Epoch 64 Step 50 Loss 1.0266 Accuracy 0.7632\n",
            "Epoch 64 Step 100 Loss 0.9875 Accuracy 0.7719\n",
            "Epoch 64 Step 150 Loss 0.9940 Accuracy 0.7711\n",
            "Epoch 64 Step 200 Loss 0.9846 Accuracy 0.7732\n",
            "Epoch 64 Step 250 Loss 0.9810 Accuracy 0.7745\n",
            "Epoch 64: Training Loss 0.9814, Training Accuracy 0.7744\n",
            "\n",
            "Start of epoch 65\n",
            "Epoch 65 Step 0 Loss 0.9722 Accuracy 0.8011\n",
            "Epoch 65 Step 50 Loss 1.0157 Accuracy 0.7660\n",
            "Epoch 65 Step 100 Loss 0.9905 Accuracy 0.7726\n",
            "Epoch 65 Step 150 Loss 0.9891 Accuracy 0.7741\n",
            "Epoch 65 Step 200 Loss 0.9770 Accuracy 0.7769\n",
            "Epoch 65 Step 250 Loss 0.9728 Accuracy 0.7782\n",
            "Epoch 65: Training Loss 0.9694, Training Accuracy 0.7789\n",
            "Saved checkpoint at epoch 65\n",
            "\n",
            "Start of epoch 66\n",
            "Epoch 66 Step 0 Loss 1.0688 Accuracy 0.7639\n",
            "Epoch 66 Step 50 Loss 0.9837 Accuracy 0.7731\n",
            "Epoch 66 Step 100 Loss 0.9663 Accuracy 0.7783\n",
            "Epoch 66 Step 150 Loss 0.9699 Accuracy 0.7780\n",
            "Epoch 66 Step 200 Loss 0.9581 Accuracy 0.7802\n",
            "Epoch 66 Step 250 Loss 0.9567 Accuracy 0.7806\n",
            "Epoch 66: Training Loss 0.9587, Training Accuracy 0.7808\n",
            "\n",
            "Start of epoch 67\n",
            "Epoch 67 Step 0 Loss 0.9312 Accuracy 0.7993\n",
            "Epoch 67 Step 50 Loss 0.9639 Accuracy 0.7807\n",
            "Epoch 67 Step 100 Loss 0.9273 Accuracy 0.7889\n",
            "Epoch 67 Step 150 Loss 0.9284 Accuracy 0.7891\n",
            "Epoch 67 Step 200 Loss 0.9223 Accuracy 0.7902\n",
            "Epoch 67 Step 250 Loss 0.9210 Accuracy 0.7906\n",
            "Epoch 67: Training Loss 0.9212, Training Accuracy 0.7907\n",
            "\n",
            "Start of epoch 68\n",
            "Epoch 68 Step 0 Loss 0.8856 Accuracy 0.8123\n",
            "Epoch 68 Step 50 Loss 0.9506 Accuracy 0.7856\n",
            "Epoch 68 Step 100 Loss 0.9220 Accuracy 0.7916\n",
            "Epoch 68 Step 150 Loss 0.9163 Accuracy 0.7925\n",
            "Epoch 68 Step 200 Loss 0.9084 Accuracy 0.7940\n",
            "Epoch 68 Step 250 Loss 0.9104 Accuracy 0.7945\n",
            "Epoch 68: Training Loss 0.9075, Training Accuracy 0.7954\n",
            "\n",
            "Start of epoch 69\n",
            "Epoch 69 Step 0 Loss 0.9339 Accuracy 0.7918\n",
            "Epoch 69 Step 50 Loss 0.9244 Accuracy 0.7928\n",
            "Epoch 69 Step 100 Loss 0.9189 Accuracy 0.7936\n",
            "Epoch 69 Step 150 Loss 0.9172 Accuracy 0.7946\n",
            "Epoch 69 Step 200 Loss 0.9045 Accuracy 0.7966\n",
            "Epoch 69 Step 250 Loss 0.9042 Accuracy 0.7969\n",
            "Epoch 69: Training Loss 0.9074, Training Accuracy 0.7963\n",
            "\n",
            "Start of epoch 70\n",
            "Epoch 70 Step 0 Loss 0.9351 Accuracy 0.8067\n",
            "Epoch 70 Step 50 Loss 0.9145 Accuracy 0.7936\n",
            "Epoch 70 Step 100 Loss 0.8887 Accuracy 0.7999\n",
            "Epoch 70 Step 150 Loss 0.8930 Accuracy 0.7988\n",
            "Epoch 70 Step 200 Loss 0.8856 Accuracy 0.8003\n",
            "Epoch 70 Step 250 Loss 0.8816 Accuracy 0.8011\n",
            "Epoch 70: Training Loss 0.8802, Training Accuracy 0.8017\n",
            "Saved checkpoint at epoch 70\n",
            "\n",
            "Start of epoch 71\n",
            "Epoch 71 Step 0 Loss 0.8981 Accuracy 0.8048\n",
            "Epoch 71 Step 50 Loss 0.9059 Accuracy 0.7967\n",
            "Epoch 71 Step 100 Loss 0.8838 Accuracy 0.8019\n",
            "Epoch 71 Step 150 Loss 0.8794 Accuracy 0.8032\n",
            "Epoch 71 Step 200 Loss 0.8738 Accuracy 0.8046\n",
            "Epoch 71 Step 250 Loss 0.8716 Accuracy 0.8053\n",
            "Epoch 71: Training Loss 0.8693, Training Accuracy 0.8054\n",
            "\n",
            "Start of epoch 72\n",
            "Epoch 72 Step 0 Loss 0.8130 Accuracy 0.8309\n",
            "Epoch 72 Step 50 Loss 0.8780 Accuracy 0.8039\n",
            "Epoch 72 Step 100 Loss 0.8637 Accuracy 0.8079\n",
            "Epoch 72 Step 150 Loss 0.8583 Accuracy 0.8091\n",
            "Epoch 72 Step 200 Loss 0.8509 Accuracy 0.8106\n",
            "Epoch 72 Step 250 Loss 0.8508 Accuracy 0.8108\n",
            "Epoch 72: Training Loss 0.8542, Training Accuracy 0.8103\n",
            "\n",
            "Start of epoch 73\n",
            "Epoch 73 Step 0 Loss 0.8668 Accuracy 0.8030\n",
            "Epoch 73 Step 50 Loss 0.8815 Accuracy 0.8040\n",
            "Epoch 73 Step 100 Loss 0.8533 Accuracy 0.8091\n",
            "Epoch 73 Step 150 Loss 0.8472 Accuracy 0.8108\n",
            "Epoch 73 Step 200 Loss 0.8378 Accuracy 0.8129\n",
            "Epoch 73 Step 250 Loss 0.8383 Accuracy 0.8132\n",
            "Epoch 73: Training Loss 0.8370, Training Accuracy 0.8131\n",
            "\n",
            "Start of epoch 74\n",
            "Epoch 74 Step 0 Loss 0.8448 Accuracy 0.8141\n",
            "Epoch 74 Step 50 Loss 0.8544 Accuracy 0.8098\n",
            "Epoch 74 Step 100 Loss 0.8333 Accuracy 0.8142\n",
            "Epoch 74 Step 150 Loss 0.8274 Accuracy 0.8160\n",
            "Epoch 74 Step 200 Loss 0.8201 Accuracy 0.8174\n",
            "Epoch 74 Step 250 Loss 0.8182 Accuracy 0.8177\n",
            "Epoch 74: Training Loss 0.8155, Training Accuracy 0.8183\n",
            "\n",
            "Start of epoch 75\n",
            "Epoch 75 Step 0 Loss 0.8581 Accuracy 0.8030\n",
            "Epoch 75 Step 50 Loss 0.8385 Accuracy 0.8151\n",
            "Epoch 75 Step 100 Loss 0.8241 Accuracy 0.8176\n",
            "Epoch 75 Step 150 Loss 0.8199 Accuracy 0.8189\n",
            "Epoch 75 Step 200 Loss 0.8155 Accuracy 0.8196\n",
            "Epoch 75 Step 250 Loss 0.8142 Accuracy 0.8197\n",
            "Epoch 75: Training Loss 0.8108, Training Accuracy 0.8207\n",
            "Saved checkpoint at epoch 75\n",
            "\n",
            "Start of epoch 76\n",
            "Epoch 76 Step 0 Loss 0.7692 Accuracy 0.8476\n",
            "Epoch 76 Step 50 Loss 0.8299 Accuracy 0.8179\n",
            "Epoch 76 Step 100 Loss 0.8121 Accuracy 0.8206\n",
            "Epoch 76 Step 150 Loss 0.8065 Accuracy 0.8219\n",
            "Epoch 76 Step 200 Loss 0.7977 Accuracy 0.8227\n",
            "Epoch 76 Step 250 Loss 0.7976 Accuracy 0.8229\n",
            "Epoch 76: Training Loss 0.7956, Training Accuracy 0.8231\n",
            "\n",
            "Start of epoch 77\n",
            "Epoch 77 Step 0 Loss 0.7927 Accuracy 0.8160\n",
            "Epoch 77 Step 50 Loss 0.8232 Accuracy 0.8171\n",
            "Epoch 77 Step 100 Loss 0.8054 Accuracy 0.8210\n",
            "Epoch 77 Step 150 Loss 0.7999 Accuracy 0.8229\n",
            "Epoch 77 Step 200 Loss 0.7966 Accuracy 0.8241\n",
            "Epoch 77 Step 250 Loss 0.7922 Accuracy 0.8253\n",
            "Epoch 77: Training Loss 0.7897, Training Accuracy 0.8258\n",
            "\n",
            "Start of epoch 78\n",
            "Epoch 78 Step 0 Loss 0.7561 Accuracy 0.8346\n",
            "Epoch 78 Step 50 Loss 0.8128 Accuracy 0.8211\n",
            "Epoch 78 Step 100 Loss 0.7925 Accuracy 0.8264\n",
            "Epoch 78 Step 150 Loss 0.7887 Accuracy 0.8268\n",
            "Epoch 78 Step 200 Loss 0.7864 Accuracy 0.8270\n",
            "Epoch 78 Step 250 Loss 0.7859 Accuracy 0.8273\n",
            "Epoch 78: Training Loss 0.7819, Training Accuracy 0.8281\n",
            "\n",
            "Start of epoch 79\n",
            "Epoch 79 Step 0 Loss 0.7517 Accuracy 0.8290\n",
            "Epoch 79 Step 50 Loss 0.8078 Accuracy 0.8207\n",
            "Epoch 79 Step 100 Loss 0.7866 Accuracy 0.8259\n",
            "Epoch 79 Step 150 Loss 0.7792 Accuracy 0.8286\n",
            "Epoch 79 Step 200 Loss 0.7705 Accuracy 0.8303\n",
            "Epoch 79 Step 250 Loss 0.7707 Accuracy 0.8308\n",
            "Epoch 79: Training Loss 0.7683, Training Accuracy 0.8313\n",
            "\n",
            "Start of epoch 80\n",
            "Epoch 80 Step 0 Loss 0.7681 Accuracy 0.8383\n",
            "Epoch 80 Step 50 Loss 0.7906 Accuracy 0.8264\n",
            "Epoch 80 Step 100 Loss 0.7816 Accuracy 0.8299\n",
            "Epoch 80 Step 150 Loss 0.7752 Accuracy 0.8317\n",
            "Epoch 80 Step 200 Loss 0.7659 Accuracy 0.8327\n",
            "Epoch 80 Step 250 Loss 0.7655 Accuracy 0.8331\n",
            "Epoch 80: Training Loss 0.7636, Training Accuracy 0.8331\n",
            "Saved checkpoint at epoch 80\n",
            "\n",
            "Start of epoch 81\n",
            "Epoch 81 Step 0 Loss 0.7194 Accuracy 0.8364\n",
            "Epoch 81 Step 50 Loss 0.7726 Accuracy 0.8304\n",
            "Epoch 81 Step 100 Loss 0.7633 Accuracy 0.8316\n",
            "Epoch 81 Step 150 Loss 0.7595 Accuracy 0.8335\n",
            "Epoch 81 Step 200 Loss 0.7625 Accuracy 0.8339\n",
            "Epoch 81 Step 250 Loss 0.7604 Accuracy 0.8344\n",
            "Epoch 81: Training Loss 0.7565, Training Accuracy 0.8349\n",
            "\n",
            "Start of epoch 82\n",
            "Epoch 82 Step 0 Loss 0.7698 Accuracy 0.8401\n",
            "Epoch 82 Step 50 Loss 0.7804 Accuracy 0.8295\n",
            "Epoch 82 Step 100 Loss 0.7603 Accuracy 0.8328\n",
            "Epoch 82 Step 150 Loss 0.7526 Accuracy 0.8360\n",
            "Epoch 82 Step 200 Loss 0.7491 Accuracy 0.8360\n",
            "Epoch 82 Step 250 Loss 0.7503 Accuracy 0.8359\n",
            "Epoch 82: Training Loss 0.7479, Training Accuracy 0.8362\n",
            "\n",
            "Start of epoch 83\n",
            "Epoch 83 Step 0 Loss 0.8209 Accuracy 0.8253\n",
            "Epoch 83 Step 50 Loss 0.7675 Accuracy 0.8322\n",
            "Epoch 83 Step 100 Loss 0.7597 Accuracy 0.8326\n",
            "Epoch 83 Step 150 Loss 0.7544 Accuracy 0.8353\n",
            "Epoch 83 Step 200 Loss 0.7453 Accuracy 0.8370\n",
            "Epoch 83 Step 250 Loss 0.7452 Accuracy 0.8368\n",
            "Epoch 83: Training Loss 0.7420, Training Accuracy 0.8374\n",
            "\n",
            "Start of epoch 84\n",
            "Epoch 84 Step 0 Loss 0.7067 Accuracy 0.8550\n",
            "Epoch 84 Step 50 Loss 0.7465 Accuracy 0.8351\n",
            "Epoch 84 Step 100 Loss 0.7545 Accuracy 0.8333\n",
            "Epoch 84 Step 150 Loss 0.7443 Accuracy 0.8371\n",
            "Epoch 84 Step 200 Loss 0.7362 Accuracy 0.8387\n",
            "Epoch 84 Step 250 Loss 0.7333 Accuracy 0.8395\n",
            "Epoch 84: Training Loss 0.7321, Training Accuracy 0.8397\n",
            "\n",
            "Start of epoch 85\n",
            "Epoch 85 Step 0 Loss 0.7040 Accuracy 0.8476\n",
            "Epoch 85 Step 50 Loss 0.7450 Accuracy 0.8378\n",
            "Epoch 85 Step 100 Loss 0.7388 Accuracy 0.8388\n",
            "Epoch 85 Step 150 Loss 0.7359 Accuracy 0.8402\n",
            "Epoch 85 Step 200 Loss 0.7363 Accuracy 0.8397\n",
            "Epoch 85 Step 250 Loss 0.7338 Accuracy 0.8401\n",
            "Epoch 85: Training Loss 0.7299, Training Accuracy 0.8408\n",
            "Saved checkpoint at epoch 85\n",
            "\n",
            "Start of epoch 86\n",
            "Epoch 86 Step 0 Loss 0.7045 Accuracy 0.8476\n",
            "Epoch 86 Step 50 Loss 0.7354 Accuracy 0.8404\n",
            "Epoch 86 Step 100 Loss 0.7356 Accuracy 0.8391\n",
            "Epoch 86 Step 150 Loss 0.7242 Accuracy 0.8420\n",
            "Epoch 86 Step 200 Loss 0.7185 Accuracy 0.8426\n",
            "Epoch 86 Step 250 Loss 0.7186 Accuracy 0.8434\n",
            "Epoch 86: Training Loss 0.7160, Training Accuracy 0.8440\n",
            "\n",
            "Start of epoch 87\n",
            "Epoch 87 Step 0 Loss 0.6600 Accuracy 0.8643\n",
            "Epoch 87 Step 50 Loss 0.7379 Accuracy 0.8419\n",
            "Epoch 87 Step 100 Loss 0.7352 Accuracy 0.8405\n",
            "Epoch 87 Step 150 Loss 0.7268 Accuracy 0.8426\n",
            "Epoch 87 Step 200 Loss 0.7188 Accuracy 0.8438\n",
            "Epoch 87 Step 250 Loss 0.7181 Accuracy 0.8442\n",
            "Epoch 87: Training Loss 0.7151, Training Accuracy 0.8448\n",
            "\n",
            "Start of epoch 88\n",
            "Epoch 88 Step 0 Loss 0.6829 Accuracy 0.8606\n",
            "Epoch 88 Step 50 Loss 0.7219 Accuracy 0.8417\n",
            "Epoch 88 Step 100 Loss 0.7247 Accuracy 0.8415\n",
            "Epoch 88 Step 150 Loss 0.7165 Accuracy 0.8441\n",
            "Epoch 88 Step 200 Loss 0.7129 Accuracy 0.8449\n",
            "Epoch 88 Step 250 Loss 0.7084 Accuracy 0.8461\n",
            "Epoch 88: Training Loss 0.7048, Training Accuracy 0.8468\n",
            "\n",
            "Start of epoch 89\n",
            "Epoch 89 Step 0 Loss 0.7097 Accuracy 0.8439\n",
            "Epoch 89 Step 50 Loss 0.7330 Accuracy 0.8420\n",
            "Epoch 89 Step 100 Loss 0.7220 Accuracy 0.8428\n",
            "Epoch 89 Step 150 Loss 0.7154 Accuracy 0.8447\n",
            "Epoch 89 Step 200 Loss 0.7165 Accuracy 0.8439\n",
            "Epoch 89 Step 250 Loss 0.7147 Accuracy 0.8449\n",
            "Epoch 89: Training Loss 0.7116, Training Accuracy 0.8455\n",
            "\n",
            "Start of epoch 90\n",
            "Epoch 90 Step 0 Loss 0.7066 Accuracy 0.8513\n",
            "Epoch 90 Step 50 Loss 0.7107 Accuracy 0.8454\n",
            "Epoch 90 Step 100 Loss 0.7160 Accuracy 0.8445\n",
            "Epoch 90 Step 150 Loss 0.7059 Accuracy 0.8469\n",
            "Epoch 90 Step 200 Loss 0.6981 Accuracy 0.8479\n",
            "Epoch 90 Step 250 Loss 0.6983 Accuracy 0.8481\n",
            "Epoch 90: Training Loss 0.6952, Training Accuracy 0.8487\n",
            "Saved checkpoint at epoch 90\n",
            "\n",
            "Start of epoch 91\n",
            "Epoch 91 Step 0 Loss 0.6877 Accuracy 0.8606\n",
            "Epoch 91 Step 50 Loss 0.7092 Accuracy 0.8467\n",
            "Epoch 91 Step 100 Loss 0.7083 Accuracy 0.8458\n",
            "Epoch 91 Step 150 Loss 0.6999 Accuracy 0.8490\n",
            "Epoch 91 Step 200 Loss 0.6925 Accuracy 0.8501\n",
            "Epoch 91 Step 250 Loss 0.6911 Accuracy 0.8504\n",
            "Epoch 91: Training Loss 0.6869, Training Accuracy 0.8512\n",
            "\n",
            "Start of epoch 92\n",
            "Epoch 92 Step 0 Loss 0.6614 Accuracy 0.8643\n",
            "Epoch 92 Step 50 Loss 0.6885 Accuracy 0.8508\n",
            "Epoch 92 Step 100 Loss 0.6932 Accuracy 0.8486\n",
            "Epoch 92 Step 150 Loss 0.6873 Accuracy 0.8509\n",
            "Epoch 92 Step 200 Loss 0.6812 Accuracy 0.8517\n",
            "Epoch 92 Step 250 Loss 0.6812 Accuracy 0.8520\n",
            "Epoch 92: Training Loss 0.6775, Training Accuracy 0.8527\n",
            "\n",
            "Start of epoch 93\n",
            "Epoch 93 Step 0 Loss 0.6562 Accuracy 0.8587\n",
            "Epoch 93 Step 50 Loss 0.7045 Accuracy 0.8475\n",
            "Epoch 93 Step 100 Loss 0.6958 Accuracy 0.8493\n",
            "Epoch 93 Step 150 Loss 0.6924 Accuracy 0.8504\n",
            "Epoch 93 Step 200 Loss 0.6899 Accuracy 0.8510\n",
            "Epoch 93 Step 250 Loss 0.6878 Accuracy 0.8519\n",
            "Epoch 93: Training Loss 0.6844, Training Accuracy 0.8523\n",
            "\n",
            "Start of epoch 94\n",
            "Epoch 94 Step 0 Loss 0.6741 Accuracy 0.8550\n",
            "Epoch 94 Step 50 Loss 0.6899 Accuracy 0.8511\n",
            "Epoch 94 Step 100 Loss 0.6933 Accuracy 0.8496\n",
            "Epoch 94 Step 150 Loss 0.6878 Accuracy 0.8516\n",
            "Epoch 94 Step 200 Loss 0.6862 Accuracy 0.8517\n",
            "Epoch 94 Step 250 Loss 0.6853 Accuracy 0.8519\n",
            "Epoch 94: Training Loss 0.6817, Training Accuracy 0.8524\n",
            "\n",
            "Start of epoch 95\n",
            "Epoch 95 Step 0 Loss 0.7375 Accuracy 0.8420\n",
            "Epoch 95 Step 50 Loss 0.6998 Accuracy 0.8505\n",
            "Epoch 95 Step 100 Loss 0.7045 Accuracy 0.8485\n",
            "Epoch 95 Step 150 Loss 0.6966 Accuracy 0.8513\n",
            "Epoch 95 Step 200 Loss 0.6940 Accuracy 0.8515\n",
            "Epoch 95 Step 250 Loss 0.6921 Accuracy 0.8518\n",
            "Epoch 95: Training Loss 0.6874, Training Accuracy 0.8528\n",
            "Saved checkpoint at epoch 95\n",
            "\n",
            "Start of epoch 96\n",
            "Epoch 96 Step 0 Loss 0.6964 Accuracy 0.8587\n",
            "Epoch 96 Step 50 Loss 0.6751 Accuracy 0.8539\n",
            "Epoch 96 Step 100 Loss 0.6851 Accuracy 0.8522\n",
            "Epoch 96 Step 150 Loss 0.6771 Accuracy 0.8546\n",
            "Epoch 96 Step 200 Loss 0.6738 Accuracy 0.8549\n",
            "Epoch 96 Step 250 Loss 0.6746 Accuracy 0.8549\n",
            "Epoch 96: Training Loss 0.6717, Training Accuracy 0.8556\n",
            "\n",
            "Start of epoch 97\n",
            "Epoch 97 Step 0 Loss 0.6930 Accuracy 0.8587\n",
            "Epoch 97 Step 50 Loss 0.6753 Accuracy 0.8534\n",
            "Epoch 97 Step 100 Loss 0.6828 Accuracy 0.8521\n",
            "Epoch 97 Step 150 Loss 0.6743 Accuracy 0.8541\n",
            "Epoch 97 Step 200 Loss 0.6692 Accuracy 0.8551\n",
            "Epoch 97 Step 250 Loss 0.6652 Accuracy 0.8564\n",
            "Epoch 97: Training Loss 0.6613, Training Accuracy 0.8570\n",
            "\n",
            "Start of epoch 98\n",
            "Epoch 98 Step 0 Loss 0.5871 Accuracy 0.8810\n",
            "Epoch 98 Step 50 Loss 0.6661 Accuracy 0.8579\n",
            "Epoch 98 Step 100 Loss 0.6768 Accuracy 0.8549\n",
            "Epoch 98 Step 150 Loss 0.6712 Accuracy 0.8561\n",
            "Epoch 98 Step 200 Loss 0.6792 Accuracy 0.8547\n",
            "Epoch 98 Step 250 Loss 0.6780 Accuracy 0.8550\n",
            "Epoch 98: Training Loss 0.6768, Training Accuracy 0.8552\n",
            "\n",
            "Start of epoch 99\n",
            "Epoch 99 Step 0 Loss 0.6716 Accuracy 0.8476\n",
            "Epoch 99 Step 50 Loss 0.6750 Accuracy 0.8549\n",
            "Epoch 99 Step 100 Loss 0.6874 Accuracy 0.8520\n",
            "Epoch 99 Step 150 Loss 0.6769 Accuracy 0.8551\n",
            "Epoch 99 Step 200 Loss 0.6745 Accuracy 0.8549\n",
            "Epoch 99 Step 250 Loss 0.6723 Accuracy 0.8559\n",
            "Epoch 99: Training Loss 0.6694, Training Accuracy 0.8562\n",
            "\n",
            "Start of epoch 100\n",
            "Epoch 100 Step 0 Loss 0.6300 Accuracy 0.8625\n",
            "Epoch 100 Step 50 Loss 0.6754 Accuracy 0.8563\n",
            "Epoch 100 Step 100 Loss 0.6897 Accuracy 0.8525\n",
            "Epoch 100 Step 150 Loss 0.6796 Accuracy 0.8551\n",
            "Epoch 100 Step 200 Loss 0.6733 Accuracy 0.8558\n",
            "Epoch 100 Step 250 Loss 0.6708 Accuracy 0.8563\n",
            "Epoch 100: Training Loss 0.6660, Training Accuracy 0.8573\n",
            "Saved checkpoint at epoch 100\n",
            "Total time taken: 110.66s\n"
          ]
        }
      ],
      "source": [
        " # Speeding up the training process\n",
        "@function\n",
        "def train_step(encoder_input, decoder_input, decoder_output):\n",
        "    with GradientTape() as tape:\n",
        "\n",
        "        # Run the forward pass of the model to generate a prediction\n",
        "        prediction = training_model(encoder_input, decoder_input, training=True)\n",
        "\n",
        "        # Compute the training loss\n",
        "        loss = loss_fcn(decoder_output, prediction)\n",
        "\n",
        "        # Compute the training accuracy\n",
        "        accuracy = accuracy_fcn(decoder_output, prediction)\n",
        "\n",
        "    # Retrieve gradients of the trainable variables with respect to the training loss\n",
        "    gradients = tape.gradient(loss, training_model.trainable_weights)\n",
        "\n",
        "    # Update the values of the trainable variables by gradient descent\n",
        "    optimizer.apply_gradients(zip(gradients, training_model.trainable_weights))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    print(\"\\nStart of epoch %d\" % (epoch + 1))\n",
        "\n",
        "    start_time = time()\n",
        "\n",
        "    # Iterate over the dataset batches\n",
        "    for step, (train_batchX, train_batchY) in enumerate(train_dataset):\n",
        "\n",
        "        # Define the encoder and decoder inputs, and the decoder output\n",
        "        encoder_input = train_batchX[:, 1:]\n",
        "        decoder_input = train_batchY[:, :-1]\n",
        "        decoder_output = train_batchY[:, 1:]\n",
        "\n",
        "        train_step(encoder_input, decoder_input, decoder_output)\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "            # print(\"Samples so far: %s\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Print epoch number and loss value at the end of every epoch\n",
        "    print(\"Epoch %d: Training Loss %.4f, Training Accuracy %.4f\" % (epoch + 1, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    # Save a checkpoint after every five epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        save_path = ckpt_manager.save()\n",
        "        print(\"Saved checkpoint at epoch %d\" % (epoch + 1))\n",
        "\n",
        "print(\"Total time taken: %.2fs\" % (time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MpzYXxidU5s"
      },
      "outputs": [],
      "source": [
        "def evaluate(text):\n",
        "    # Tokenize the input text using the eng_tokenizer\n",
        "    text = eng_tokenizer.texts_to_sequences([text])\n",
        "    # Pad the tokenized sequence to a fixed length using the tf.keras.preprocessing.sequence.pad_sequences function\n",
        "    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
        "    # Expand the shape of the tokenized and padded sequence to match the input shape of the Transformer encoder\n",
        "    encoder_input = tf.expand_dims(text[0], 0)\n",
        "    # Initialize the decoder input with the <sos> token\n",
        "    decoder_input = [hind_tokenizer.word_index['<sos>']]\n",
        "    # Expand the shape of the decoder input to match the input shape of the Transformer decoder\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    # Loop through the length of the decoder sequence\n",
        "    for i in range(DECODER_LEN):\n",
        "        # Generate the masks for the Transformer model using the create_masks function\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "        # Pass the encoder and decoder inputs through the Transformer model to generate predictions and attention weights\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "        # Select the last predicted token from the prediction sequence\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        # Convert the predicted token to an integer using the argmax function\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # If the predicted token is the <eos> token, return the predicted sequence and attention weights\n",
        "        if predicted_id == hind_tokenizer.word_index['<eos>']:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "        # Append the predicted token to the output sequence\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    # If the predicted sequence has not ended with the <eos> token, return the predicted sequence and attention weights\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8BOwf2mPo1E"
      },
      "outputs": [],
      "source": [
        "\n",
        "#training_model.save('my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfHCQ91EP6eE"
      },
      "outputs": [],
      "source": [
        "#new_model = tf.keras.models.load_model('my_model.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}